{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cpu_gpu_code.csv')\n",
    "\n",
    "# Create target labels\n",
    "df['target'] = np.where(df['CPU runing time'] < df['GPU runing time'], 0, 1)\n",
    "\n",
    "# Combine CPU assembly and GPU PTX code\n",
    "df['code'] = df['CPU assembly code '] + ' ' + df['GPU ptx code']\n",
    "\n",
    "# Tokenize the code\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['code'])\n",
    "sequences = tokenizer.texts_to_sequences(df['code'])\n",
    "\n",
    "# Set the parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 128\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split the dataset\n",
    "x_train, x_val, y_train, y_val = train_test_split(padded_sequences, df['target'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Save the tokenizer and model\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "model.save('cpu_gpu_predictor.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 981ms/step\n",
      "Choose CPU code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "model = load_model('cpu_gpu_predictor.h5')\n",
    "\n",
    "# Input: new CPU assembly code and GPU ptx code\n",
    "cpu_code = '''\n",
    "; Increment each element of an array by one in x86 assembly\n",
    "; void increment_array(int* array, int size);\n",
    "\n",
    "section .text\n",
    "global increment_array\n",
    "increment_array:\n",
    "    ; Function prologue\n",
    "    push ebp\n",
    "    mov ebp, esp\n",
    "\n",
    "    ; Load parameters\n",
    "    mov eax, [ebp+8] ; array\n",
    "    mov ecx, [ebp+12] ; size\n",
    "\n",
    "    ; Loop\n",
    "    xor edx, edx ; i = 0\n",
    ".loop:\n",
    "    cmp edx, ecx\n",
    "    jge .end_loop\n",
    "\n",
    "    ; Increment array element by one\n",
    "    add dword [eax + edx*4], 1\n",
    "\n",
    "    ; Increment loop counter\n",
    "    inc edx\n",
    "    jmp .loop\n",
    "\n",
    ".end_loop:\n",
    "    ; Function epilogue\n",
    "    pop ebp\n",
    "    ret\n",
    "'''\n",
    "\n",
    "gpu_code = '''\n",
    ".version 6.4\n",
    ".target sm_30\n",
    ".address_size 64\n",
    "\n",
    ".entry increment_array_gpu (\n",
    "    .param .u64 increment_array_gpu_param_0, ; int* array\n",
    "    .param .u32 increment_array_gpu_param_1  ; int size\n",
    ")\n",
    "{\n",
    "    .reg .b32 %r<3>;\n",
    "    .reg .b64 %rd<3>;\n",
    "\n",
    "    ; Load parameters\n",
    "    ld.param.u64 %rd1, increment_array_gpu_param_0;\n",
    "    ld.param.u32 %r1, increment_array_gpu_param_1;\n",
    "\n",
    "    ; Get the thread index\n",
    "    mov.u32 %r2, %tid.x;\n",
    "\n",
    "    ; Check if the thread index is within the array size\n",
    "    setp.ge.u32 %p1, %r2, %r1;\n",
    "    @%p1 exit;\n",
    "\n",
    "    ; Increment array element by one\n",
    "    atomic.add.s32 [%rd1 + %r2 * 4], 1;\n",
    "\n",
    "    exit:;\n",
    "    ret;\n",
    "}\n",
    "'''\n",
    "\n",
    "# Preprocess the input data\n",
    "input_data = cpu_code + ' ' + gpu_code\n",
    "input_sequence = tokenizer.texts_to_sequences([input_data])\n",
    "padded_input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "prediction = model.predict(padded_input_sequence)\n",
    "\n",
    "# Choose the best option (CPU or GPU) based on the prediction\n",
    "if prediction[0][0] < 0.5:\n",
    "    print(\"Choose CPU code\")\n",
    "else:\n",
    "    print(\"Choose GPU code\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
